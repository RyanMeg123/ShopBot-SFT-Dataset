#!/usr/bin/env python3
"""
ShopBot SFT è®­ç»ƒè„šæœ¬
åŸºäº Hugging Face TRL åº“å®ç°æœ‰ç›‘ç£å¾®è°ƒ
å…¼å®¹ TRL >= 0.8.0
"""

import json
import os
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig, TaskType, get_peft_model

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# ============ é…ç½®åŒºåŸŸ ============

# æ¨¡å‹é…ç½®
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "outputs/sft_model")
DATA_PATH = os.path.join(PROJECT_ROOT, "data/sft/train_v1.jsonl")

# LoRAé…ç½®
LORA_RANK = 8
LORA_ALPHA = 32
LORA_DROPOUT = 0.1

# è®­ç»ƒé…ç½®
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 4
NUM_EPOCHS = 20  # å°æ•°æ®é›†éœ€è¦æ›´å¤šè½®æ•°
LEARNING_RATE = 2e-4
MAX_SEQ_LENGTH = 512

# ==================================


def load_data(data_path):
    """åŠ è½½SFTæ•°æ®"""
    data = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append(item)
    print(f"âœ… åŠ è½½äº† {len(data)} æ¡è®­ç»ƒæ•°æ®")
    return data


def format_conversation(example):
    """å°†å¯¹è¯æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥"""
    conversation = example["conversation"]
    # ä½¿ç”¨æ¨¡å‹æŒ‡å®šçš„chat template
    formatted = tokenizer.apply_chat_template(
        conversation,
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": formatted}


def main():
    global tokenizer
    
    print("ğŸš€ å¼€å§‹ ShopBot SFT è®­ç»ƒ")
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {BASE_MODEL}")
    print(f"ğŸ“Š æ•°æ®è·¯å¾„: {DATA_PATH}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. åŠ è½½tokenizerå’Œæ¨¡å‹
    print("\nâ³ åŠ è½½æ¨¡å‹å’Œtokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
        padding_side="right"
    )
    
    # è®¾ç½®pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    
    # 2. é…ç½®LoRA
    print("\nâ³ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=LORA_RANK,
        lora_alpha=LORA_ALPHA,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    print("âœ… LoRAé…ç½®å®Œæˆ")
    
    # 3. åŠ è½½æ•°æ®
    print("\nâ³ åŠ è½½è®­ç»ƒæ•°æ®...")
    raw_data = load_data(DATA_PATH)
    
    # è½¬æ¢ä¸ºHugging Face Datasetæ ¼å¼
    dataset = Dataset.from_list(raw_data)
    
    # æ ¼å¼åŒ–æ•°æ®
    dataset = dataset.map(format_conversation)
    print(f"âœ… æ•°æ®æ ¼å¼åŒ–å®Œæˆï¼Œç¤ºä¾‹ï¼š")
    print(f"   {dataset[0]['text'][:200]}...")
    
    # 4. é…ç½®SFTConfig (æ–°ç‰ˆTRLä½¿ç”¨SFTConfig)
    print("\nâ³ é…ç½®è®­ç»ƒ...")
    sft_config = SFTConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        warmup_steps=10,
        logging_steps=5,
        save_steps=20,
        save_total_limit=2,
        bf16=True,  # Mac MPS æ”¯æŒ bf16ï¼Œä¸æ”¯æŒ fp16
        report_to="none",
        max_length=MAX_SEQ_LENGTH,
    )
    
    # 5. åˆ›å»ºTrainer (æ–°ç‰ˆAPI: argsç”¨SFTConfig, processing_classä»£æ›¿tokenizer)
    trainer = SFTTrainer(
        model=model,
        processing_class=tokenizer,
        train_dataset=dataset,
        args=sft_config,
    )
    print("âœ… Traineråˆ›å»ºå®Œæˆ")
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("\nğŸ¬ å¼€å§‹è®­ç»ƒï¼")
    trainer.train()
    
    # 7. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model(os.path.join(OUTPUT_DIR, "final"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final"))
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}/final")
    
    # 8. ä¿å­˜è®­ç»ƒé…ç½®
    config_info = {
        "base_model": BASE_MODEL,
        "lora_rank": LORA_RANK,
        "lora_alpha": LORA_ALPHA,
        "batch_size": BATCH_SIZE,
        "num_epochs": NUM_EPOCHS,
        "learning_rate": LEARNING_RATE,
        "data_size": len(raw_data)
    }
    with open(os.path.join(OUTPUT_DIR, "training_config.json"), "w") as f:
        json.dump(config_info, f, indent=2)
    
    print("\nğŸ‰ SFTè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(raw_data)} æ¡")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {NUM_EPOCHS} è½®")


if __name__ == "__main__":
    main()
