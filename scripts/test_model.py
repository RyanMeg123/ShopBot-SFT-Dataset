#!/usr/bin/env python3
"""
ShopBot SFT æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹æ•ˆæœ
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import sys

# æ¨¡å‹è·¯å¾„
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
ADAPTER_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "outputs/sft_model/final")


def load_model():
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    print("â³ åŠ è½½æ¨¡å‹...")
    
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(model, ADAPTER_PATH)
    model = model.merge_and_unload()  # åˆå¹¶æƒé‡åŠ é€Ÿæ¨ç†
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer


def chat(model, tokenizer, user_input):
    """ä¸æ¨¡å‹å¯¹è¯"""
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†å®¢æœåŠ©æ‰‹ï¼Œçƒ­æƒ…ã€è€å¿ƒåœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
        {"role": "user", "content": user_input}
    ]
    
    # åº”ç”¨chat template
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œæ›´æ¥è¿‘è®­ç»ƒæ•°æ®
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True
        )
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–åŠ©æ‰‹å›å¤ï¼ˆå»æ‰promptéƒ¨åˆ†ï¼‰
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    return response


def main():
    print("=" * 50)
    print("ğŸ¤– ShopBot å®¢æœåŠ©æ‰‹æµ‹è¯•")
    print("=" * 50)
    
    try:
        model, tokenizer = load_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("æç¤ºï¼šè¯·å…ˆè¿è¡Œ sft_train.py å®Œæˆè®­ç»ƒ")
        sys.exit(1)
    
    # é¢„è®¾æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ä½ å¥½ï¼Œè¿™ä»¶Tæ¤æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
        "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ",
        "è¿™ä¸ªé‹å­å¤ªå¤§äº†ï¼Œæƒ³é€€",
        "ç°åœ¨æœ‰ä»€ä¹ˆä¼˜æƒ å—ï¼Ÿ",
    ]
    
    print("\nğŸ“‹ é¢„è®¾æµ‹è¯•ç”¨ä¾‹ï¼š")
    for i, test in enumerate(test_cases, 1):
        print(f"  {i}. {test}")
    
    print("\nğŸ’¡ è¾“å…¥æ•°å­—(1-4)é€‰æ‹©æµ‹è¯•ç”¨ä¾‹ï¼Œæˆ–ç›´æ¥è¾“å…¥é—®é¢˜")
    print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡º")
    print("-" * 50)
    
    while True:
        user_input = input("\nğŸ“ è¾“å…¥: ").strip()
        
        if user_input.lower() in ["quit", "exit", "q"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        # å¤„ç†æ•°å­—é€‰æ‹©
        if user_input.isdigit() and 1 <= int(user_input) <= len(test_cases):
            user_input = test_cases[int(user_input) - 1]
            print(f"ğŸ“ é—®é¢˜: {user_input}")
        
        print("â³ ç”Ÿæˆå›å¤...")
        response = chat(model, tokenizer, user_input)
        print(f"ğŸ¤– å›å¤: {response}")


if __name__ == "__main__":
    main()
