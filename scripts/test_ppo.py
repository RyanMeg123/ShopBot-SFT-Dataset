#!/usr/bin/env python3
"""
ShopBot PPO æ¨¡å‹æµ‹è¯•è„šæœ¬
å¯¹æ¯” SFTæ¨¡å‹ vs PPOæ¨¡å‹ çš„æ•ˆæœ
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# æ¨¡å‹è·¯å¾„
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
SFT_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/sft_model/final")
PPO_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/ppo_model/final")
REWARD_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/reward_model/final")


def load_model_for_comparison(model_path, model_name, device):
    """åŠ è½½æ¨¡å‹"""
    print(f"\nâ³ åŠ è½½ {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, 
        trust_remote_code=True,
        local_files_only=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float32,
        trust_remote_code=True
    ).to(device)
    
    model = PeftModel.from_pretrained(model, model_path)
    model.eval()
    
    print(f"âœ… {model_name} åŠ è½½å®Œæˆ")
    return model, tokenizer


def generate_response(model, tokenizer, prompt, device, max_new_tokens=100):
    """ç”Ÿæˆå›ç­”"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(text, return_tensors="pt", return_attention_mask=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()


def get_reward_score(rm_model, rm_tokenizer, prompt, response, device):
    """RMæ‰“åˆ†"""
    text = f"{prompt}\n{response}"
    inputs = rm_tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
    
    with torch.no_grad():
        outputs = rm_model(**inputs)
        score = outputs.logits[0].item()
    
    return score


def compare_models(sft_model, sft_tokenizer, ppo_model, ppo_tokenizer, 
                   rm_model, rm_tokenizer, prompt, device):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”"""
    
    # SFTç”Ÿæˆ
    sft_response = generate_response(sft_model, sft_tokenizer, prompt, device)
    sft_score = get_reward_score(rm_model, rm_tokenizer, prompt, sft_response, device)
    
    # PPOç”Ÿæˆ
    ppo_response = generate_response(ppo_model, ppo_tokenizer, prompt, device)
    ppo_score = get_reward_score(rm_model, rm_tokenizer, prompt, ppo_response, device)
    
    return {
        "prompt": prompt,
        "sft_response": sft_response,
        "sft_score": sft_score,
        "ppo_response": ppo_response,
        "ppo_score": ppo_score
    }


def print_comparison(result, idx):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ã€æµ‹è¯• {idx}ã€‘{result['prompt']}")
    print(f"{'='*60}")
    
    print(f"\nğŸ“ SFTæ¨¡å‹å›ç­”:")
    print(f"   {result['sft_response'][:100]}{'...' if len(result['sft_response']) > 100 else ''}")
    print(f"   â­ RMåˆ†æ•°: {result['sft_score']:+.2f}")
    
    print(f"\nğŸ“ PPOæ¨¡å‹å›ç­”:")
    print(f"   {result['ppo_response'][:100]}{'...' if len(result['ppo_response']) > 100 else ''}")
    print(f"   â­ RMåˆ†æ•°: {result['ppo_score']:+.2f}")
    
    # åˆ¤æ–­å“ªä¸ªæ›´å¥½
    if result['ppo_score'] > result['sft_score']:
        diff = result['ppo_score'] - result['sft_score']
        print(f"\nğŸ† ç»“æœ: PPOæ›´ä¼˜ (é«˜ {diff:.2f} åˆ†)")
    elif result['sft_score'] > result['ppo_score']:
        diff = result['sft_score'] - result['ppo_score']
        print(f"\nğŸ† ç»“æœ: SFTæ›´ä¼˜ (é«˜ {diff:.2f} åˆ†)")
    else:
        print(f"\nâš–ï¸ ç»“æœ: ä¸¤è€…ç›¸å½“")


def main():
    print("="*60)
    print("ğŸ¤– ShopBot æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    print("å¯¹æ¯”: SFTæ¨¡å‹ vs PPOæ¨¡å‹")
    print("="*60)
    
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    try:
        sft_model, sft_tokenizer = load_model_for_comparison(SFT_MODEL_PATH, "SFTæ¨¡å‹", device)
        ppo_model, ppo_tokenizer = load_model_for_comparison(PPO_MODEL_PATH, "PPOæ¨¡å‹", device)
        
        # åŠ è½½RM
        print("\nâ³ åŠ è½½Reward Model...")
        rm_tokenizer = AutoTokenizer.from_pretrained(
            REWARD_MODEL_PATH, 
            trust_remote_code=True,
            local_files_only=True
        )
        rm_model = AutoModelForSequenceClassification.from_pretrained(
            REWARD_MODEL_PATH,
            num_labels=1,
            trust_remote_code=True,
            local_files_only=True
        ).to(device)
        rm_model.eval()
        print("âœ… Reward Model åŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®è®¤ä»¥ä¸‹æ¨¡å‹å·²è®­ç»ƒ:")
        print(f"  - {SFT_MODEL_PATH}")
        print(f"  - {PPO_MODEL_PATH}")
        print(f"  - {REWARD_MODEL_PATH}")
        return
    
    # æµ‹è¯•ç”¨ä¾‹
    test_prompts = [
        "ä½ å¥½ï¼Œè¿™ä»¶Tæ¤æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
        "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ",
        "è¿™ä¸ªé‹å­å¤ªå¤§äº†ï¼Œæƒ³é€€",
        "ç°åœ¨æœ‰ä»€ä¹ˆä¼˜æƒ å—ï¼Ÿ",
        "å‘è´§å¤ªæ…¢äº†ï¼Œèƒ½å¿«ç‚¹å—",
        "è¿™ä»¶è¡£æœé¢œè‰²å¤ªæ·±äº†",
        "æ€ä¹ˆå–æ¶ˆè®¢å•ï¼Ÿ",
        "é€€è´§çš„é’±å¤šä¹…åˆ°è´¦ï¼Ÿ",
    ]
    
    print("\n" + "="*60)
    print("ğŸ“Š å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    
    results = []
    for i, prompt in enumerate(test_prompts, 1):
        result = compare_models(
            sft_model, sft_tokenizer,
            ppo_model, ppo_tokenizer,
            rm_model, rm_tokenizer,
            prompt, device
        )
        results.append(result)
        print_comparison(result, i)
    
    # ç»Ÿè®¡æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“ˆ æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    ppo_wins = sum(1 for r in results if r['ppo_score'] > r['sft_score'])
    sft_wins = sum(1 for r in results if r['sft_score'] > r['ppo_score'])
    ties = len(results) - ppo_wins - sft_wins
    
    print(f"\næ€»æµ‹è¯•æ•°: {len(results)}")
    print(f"PPOèƒœå‡º: {ppo_wins} æ¬¡")
    print(f"SFTèƒœå‡º: {sft_wins} æ¬¡")
    print(f"å¹³å±€: {ties} æ¬¡")
    
    avg_sft = sum(r['sft_score'] for r in results) / len(results)
    avg_ppo = sum(r['ppo_score'] for r in results) / len(results)
    
    print(f"\nå¹³å‡åˆ†æ•°:")
    print(f"  SFT: {avg_sft:+.2f}")
    print(f"  PPO: {avg_ppo:+.2f}")
    
    if avg_ppo > avg_sft:
        print(f"\nğŸ‰ ç»“è®º: PPOæ¨¡å‹å¹³å‡è¡¨ç°æ›´ä¼˜ (é«˜ {avg_ppo - avg_sft:.2f} åˆ†)")
    else:
        print(f"\nğŸ“ ç»“è®º: SFTæ¨¡å‹å¹³å‡è¡¨ç°æ›´ä¼˜ (é«˜ {avg_sft - avg_ppo:.2f} åˆ†)")
    
    # äº¤äº’æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ’¡ äº¤äº’æµ‹è¯•æ¨¡å¼")
    print("è¾“å…¥ä½ è‡ªå·±çš„promptï¼Œå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("="*60)
    
    while True:
        print()
        prompt = input("ğŸ“ Prompt: ").strip()
        if prompt.lower() == 'quit':
            break
        if not prompt:
            continue
        
        print("\nâ³ ç”Ÿæˆä¸­...")
        result = compare_models(
            sft_model, sft_tokenizer,
            ppo_model, ppo_tokenizer,
            rm_model, rm_tokenizer,
            prompt, device
        )
        print_comparison(result, "äº¤äº’")


if __name__ == "__main__":
    main()
