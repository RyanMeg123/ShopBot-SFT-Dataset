#!/usr/bin/env python3
"""
Reward Model æµ‹è¯•è„šæœ¬ - æœ€ç»ˆç‰ˆ
"""

import os
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
REWARD_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/reward_model/final")


def load_reward_model():
    """åŠ è½½è®­ç»ƒå¥½çš„Reward Model"""
    print("â³ åŠ è½½Reward Model...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        REWARD_MODEL_PATH, 
        trust_remote_code=True,
        local_files_only=True
    )
    
    model = AutoModelForSequenceClassification.from_pretrained(
        REWARD_MODEL_PATH,
        num_labels=1,
        torch_dtype=torch.float32,
        trust_remote_code=True,
        local_files_only=True
    )
    model.eval()
    
    print("âœ… Reward ModelåŠ è½½å®Œæˆï¼")
    return model, tokenizer


def get_reward_score(model, tokenizer, prompt, response):
    """ç»™å•ä¸ªå›ç­”æ‰“åˆ†"""
    text = f"{prompt}\n{response}"
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        score = outputs.logits[0].item()
    
    return score


def main():
    print("=" * 50)
    print("ğŸ¤– Reward Model æ‰“åˆ†æµ‹è¯•")
    print("=" * 50)
    
    model, tokenizer = load_reward_model()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "prompt": "ä½ å¥½ï¼Œè¿™ä»¶Tæ¤æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
            "good": "æ‚¨å¥½ï¼è¿™æ¬¾Tæ¤ç›®å‰æœ‰é»‘è‰²ã€ç™½è‰²ã€ç°è‰²å’Œè—é’è‰²å››ç§é¢œè‰²å¯é€‰ã€‚è¯·é—®æ‚¨éœ€è¦ä»€ä¹ˆé¢œè‰²å‘¢ï¼Ÿ",
            "bad": "ä¸çŸ¥é“ï¼Œè‡ªå·±çœ‹ç½‘ç«™ã€‚"
        },
        {
            "prompt": "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ",
            "good": "æ‚¨å¥½ï¼Œæˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢ã€‚è®¢å•å·²å‘è´§ï¼Œé¢„è®¡æ˜å¤©ä¸‹åˆé€è¾¾ã€‚",
            "bad": "ç­‰ç€å§ï¼Œåˆ°äº†å°±åˆ°äº†ã€‚"
        },
        {
            "prompt": "è¿™ä¸ªé‹å­å¤ªå¤§äº†ï¼Œæƒ³é€€",
            "good": "å¥½çš„ï¼Œæˆ‘ä»¬æ”¯æŒ7å¤©æ— ç†ç”±é€€æ¢è´§ã€‚è¯·é—®æ‚¨æƒ³é€€è´§è¿˜æ˜¯æ¢å°ä¸€ç ï¼Ÿ",
            "bad": "é€€ä¸äº†ï¼Œç©¿è¿‡äº†ä¸èƒ½é€€ã€‚"
        }
    ]
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print("=" * 50)
    
    all_correct = True
    for i, test in enumerate(test_cases, 1):
        score_good = get_reward_score(model, tokenizer, test["prompt"], test["good"])
        score_bad = get_reward_score(model, tokenizer, test["prompt"], test["bad"])
        
        print(f"\nã€æµ‹è¯• {i}ã€‘{test['prompt'][:30]}...")
        print(f"  å¥½å›ç­”: {score_good:+.2f}")
        print(f"  å·®å›ç­”: {score_bad:+.2f}")
        
        if score_good > score_bad:
            diff = score_good - score_bad
            print(f"  âœ… æ­£ç¡® (å¥½å›ç­”é«˜ {diff:.2f} åˆ†)")
        else:
            print(f"  âŒ é”™è¯¯ (å·®å›ç­”æ›´é«˜)")
            all_correct = False
    
    print("\n" + "=" * 50)
    if all_correct:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Reward Model å·¥ä½œæ­£å¸¸")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œæ¨¡å‹éœ€è¦æ›´å¤šè®­ç»ƒ")
    print("=" * 50)
    
    # äº¤äº’æµ‹è¯•
    print("\nğŸ’¡ äº¤äº’æµ‹è¯•æ¨¡å¼")
    print("   è¾“å…¥ prompt å’Œå›ç­”ï¼ŒæŸ¥çœ‹ RM åˆ†æ•°")
    print("   è¾“å…¥ 'quit' é€€å‡º")
    
    while True:
        print()
        prompt = input("ğŸ“ Prompt: ").strip()
        if prompt.lower() == 'quit':
            break
        if not prompt:
            continue
            
        response = input("ğŸ“ å›ç­”: ").strip()
        if response.lower() == 'quit':
            break
        if not response:
            continue
        
        score = get_reward_score(model, tokenizer, prompt, response)
        print(f"   â­ RM åˆ†æ•°: {score:+.2f}")
        
        if score > 0:
            print("   ğŸ‘ æ­£é¢è¯„ä»·")
        elif score > -5:
            print("   ğŸ˜ ä¸­æ€§è¯„ä»·")
        else:
            print("   ğŸ‘ è´Ÿé¢è¯„ä»·")


if __name__ == "__main__":
    main()
