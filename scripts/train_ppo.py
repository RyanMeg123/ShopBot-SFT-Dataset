#!/usr/bin/env python3
"""
ShopBot PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬ - ç®€åŒ–ç‰ˆ
ç”¨Reward Modelçš„åé¦ˆæ¥ä¼˜åŒ–SFTæ¨¡å‹
"""

import json
import os
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from peft import PeftModel

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# ============ é…ç½®åŒºåŸŸ ============

BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
SFT_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/sft_model/final")
REWARD_MODEL_PATH = os.path.join(PROJECT_ROOT, "outputs/reward_model/final")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "outputs/ppo_model")

NUM_STEPS = 10  # æ¼”ç¤ºç”¨çš„æ­¥æ•°
MAX_NEW_TOKENS = 100

# ==================================


def load_prompts():
    """åŠ è½½ç”¨äºè®­ç»ƒçš„prompts"""
    prompts = [
        "ä½ å¥½ï¼Œè¿™ä»¶Tæ¤æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
        "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ",
        "è¿™ä¸ªé‹å­å¤ªå¤§äº†ï¼Œæƒ³é€€",
        "ç°åœ¨æœ‰ä»€ä¹ˆä¼˜æƒ å—ï¼Ÿ",
        "å‘è´§å¤ªæ…¢äº†ï¼Œèƒ½å¿«ç‚¹å—",
    ]
    return prompts


def get_reward(rm_model, rm_tokenizer, query, response, device):
    """è®¡ç®—å¥–åŠ±åˆ†æ•°"""
    text = f"{query}\n{response}"
    inputs = rm_tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = rm_model(**inputs)
        score = outputs.logits[0].item()
    return score


def generate_response(model, tokenizer, prompt, device, max_new_tokens=100):
    """ç”Ÿæˆå›ç­”"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(text, return_tensors="pt", return_attention_mask=True).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()


def simple_ppo_step(model, ref_model, optimizer, query, old_response, reward, kl_coef=0.2):
    """
    ç®€åŒ–çš„PPOå•æ­¥æ›´æ–°
    å®é™…PPOæ›´å¤æ‚ï¼Œè¿™é‡Œæ¼”ç¤ºæ ¸å¿ƒæ€æƒ³
    """
    # æ³¨æ„ï¼šè¿™æ˜¯æåº¦ç®€åŒ–çš„ç‰ˆæœ¬ï¼ŒçœŸå®PPOéœ€è¦è®¡ç®—ä¼˜åŠ¿å‡½æ•°ã€é‡è¦æ€§é‡‡æ ·ç­‰
    # è¿™é‡Œåªåšæ¦‚å¿µæ¼”ç¤º
    
    device = next(model.parameters()).device
    
    messages = [{"role": "user", "content": query}]
    prompt_text = model.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # ç¼–ç å®Œæ•´çš„prompt+response
    full_text = prompt_text + old_response
    inputs = model.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=512).to(device)
    
    # å‰å‘ä¼ æ’­
    outputs = model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss
    
    # ç”¨rewardä½œä¸ºlossçš„æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # çœŸå®PPOè¿™é‡Œä¼šå¤æ‚å¾—å¤š
    weighted_loss = loss * (1.0 - torch.sigmoid(torch.tensor(reward / 10.0)).item())
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    weighted_loss.backward()
    optimizer.step()
    
    return weighted_loss.item()


def main():
    print("=" * 60)
    print("ğŸš€ ShopBot PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒ (ç®€åŒ–ç‰ˆ)")
    print("=" * 60)
    print("ğŸ’¡ PPO = ç”¨RMçš„åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹")
    print("ğŸ’¡ æ³¨æ„ï¼šè¿™æ˜¯æ•™å­¦ç®€åŒ–ç‰ˆï¼Œéç”Ÿäº§çº§å®ç°")
    print("=" * 60)
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ¨¡å‹
    print("\nâ³ åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True, local_files_only=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆè¦è®­ç»ƒçš„ï¼‰
    print("  åŠ è½½ç­–ç•¥æ¨¡å‹(SFT)...")
    policy_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float32,
        trust_remote_code=True
    ).to(device)
    policy_model = PeftModel.from_pretrained(policy_model, SFT_MODEL_PATH)
    policy_model.train()
    
    # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼Œç”¨äºè®¡ç®—KLæ•£åº¦ï¼‰
    print("  åŠ è½½å‚è€ƒæ¨¡å‹(å†»ç»“)...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float32,
        trust_remote_code=True
    ).to(device)
    ref_model = PeftModel.from_pretrained(ref_model, SFT_MODEL_PATH)
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False
    
    # åŠ è½½Reward Model
    print("  åŠ è½½Reward Model...")
    rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True, local_files_only=True)
    rm_model = AutoModelForSequenceClassification.from_pretrained(
        REWARD_MODEL_PATH,
        num_labels=1,
        trust_remote_code=True,
        local_files_only=True
    ).to(device)
    rm_model.eval()
    
    # ç»™æ¨¡å‹é™„åŠ tokenizerï¼ˆç”¨äºgenerateï¼‰
    policy_model.tokenizer = tokenizer
    
    print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. å‡†å¤‡ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)
    
    # 3. åŠ è½½prompts
    prompts = load_prompts()
    print(f"\nğŸ“Š è®­ç»ƒæ•°æ®: {len(prompts)} ä¸ªprompts")
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\nğŸ¬ å¼€å§‹PPOè®­ç»ƒï¼")
    print("-" * 60)
    
    for step in range(NUM_STEPS):
        prompt = prompts[step % len(prompts)]
        
        print(f"\nã€Step {step + 1}/{NUM_STEPS}ã€‘")
        print(f"  Prompt: {prompt}")
        
        # ç”Ÿæˆå›ç­”
        response = generate_response(policy_model, tokenizer, prompt, device, MAX_NEW_TOKENS)
        print(f"  ç”Ÿæˆå›ç­”: {response[:60]}...")
        
        # è®¡ç®—å¥–åŠ±
        reward = get_reward(rm_model, rm_tokenizer, prompt, response, device)
        print(f"  RMè¯„åˆ†: {reward:+.2f}")
        
        # ç®€å•PPOæ›´æ–°ï¼ˆæ•™å­¦ç‰ˆï¼‰
        try:
            loss = simple_ppo_step(policy_model, ref_model, optimizer, prompt, response, reward)
            print(f"  æ›´æ–°æŸå¤±: {loss:.4f}")
            print(f"  âœ… æ¨¡å‹å·²æ›´æ–°ï¼ˆå‘é«˜åˆ†æ–¹å‘ä¼˜åŒ–ï¼‰")
        except Exception as e:
            print(f"  âš ï¸ æ›´æ–°è·³è¿‡: {e}")
        
        print("-" * 60)
    
    # 5. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜PPOæ¨¡å‹...")
    policy_model.save_pretrained(os.path.join(OUTPUT_DIR, "final"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final"))
    
    # ä¿å­˜é…ç½®
    config_info = {
        "base_model": BASE_MODEL,
        "sft_model": SFT_MODEL_PATH,
        "reward_model": REWARD_MODEL_PATH,
        "num_steps": NUM_STEPS,
        "note": "ç®€åŒ–ç‰ˆPPOï¼Œä»…ç”¨äºæ•™å­¦æ¼”ç¤º"
    }
    with open(os.path.join(OUTPUT_DIR, "ppo_config.json"), "w") as f:
        json.dump(config_info, f, indent=2)
    
    print("\nğŸ‰ PPOè®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print("ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ:")
    print("   1. æ¨¡å‹ç”Ÿæˆå›ç­”")
    print("   2. RMç»™å›ç­”æ‰“åˆ†")
    print("   3. é«˜åˆ†å›ç­” â†’ å¼ºåŒ–å­¦ä¹  â†’ æ¨¡å‹æ›´æ–°")
    print("   4. é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œæ¨¡å‹è¶Šæ¥è¶Šä¼šç”Ÿæˆé«˜åˆ†å›ç­”")
    print("=" * 60)
    
    print("\nğŸ“‚ æ¨¡å‹å·²ä¿å­˜åˆ°:", os.path.join(OUTPUT_DIR, "final"))


if __name__ == "__main__":
    main()
