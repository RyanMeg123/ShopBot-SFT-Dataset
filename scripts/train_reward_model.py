#!/usr/bin/env python3
"""
ShopBot Reward Model è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆ
å…³é”®ä¿®å¤ï¼šåˆ†ç±»å±‚(score)éœ€è¦å•ç‹¬è®­ç»ƒï¼Œä¸åœ¨LoRAé‡Œ
"""

import json
import os
import torch
from datasets import Dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from peft import LoraConfig, TaskType, get_peft_model
from trl import RewardTrainer, RewardConfig

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# ============ é…ç½®åŒºåŸŸ ============

BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "outputs/reward_model")
DATA_PATH = os.path.join(PROJECT_ROOT, "data/rlhf/preference_pairs_v1.jsonl")

LORA_RANK = 8
LORA_ALPHA = 32

BATCH_SIZE = 2
NUM_EPOCHS = 10
LEARNING_RATE = 1e-4
MAX_LENGTH = 512

# ==================================


def load_preference_data(data_path):
    """åŠ è½½åå¥½å¯¹æ¯”æ•°æ®"""
    data = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append({
                "prompt": item["prompt"],
                "chosen": item["chosen"],
                "rejected": item["rejected"]
            })
    print(f"âœ… åŠ è½½äº† {len(data)} å¯¹åå¥½æ•°æ®")
    return data


def main():
    print("ğŸš€ å¼€å§‹ ShopBot Reward Model è®­ç»ƒ")
    print("=" * 50)
    print("ğŸ’¡ å…³é”®ï¼šLoRAåªè®­ç»ƒTransformerå±‚ï¼Œscoreå±‚å…¨é‡è®­ç»ƒ")
    print("=" * 50)
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. åŠ è½½tokenizer
    print("\nâ³ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ… TokenizeråŠ è½½å®Œæˆ")
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print("\nâ³ åŠ è½½åŸºç¡€æ¨¡å‹...")
    model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL,
        num_labels=1,
        torch_dtype=torch.float32,  # ç”¨float32æ›´ç¨³å®š
        device_map="auto",
        trust_remote_code=True
    )
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. å…³é”®ä¿®å¤ï¼šåªç»™transformerå±‚åŠ LoRAï¼Œscoreå±‚ä¿æŒå¯è®­ç»ƒ
    print("\nâ³ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=LORA_RANK,
        lora_alpha=LORA_ALPHA,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # ä¸åŒ…æ‹¬score
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.SEQ_CLS,
        # å…³é”®ï¼šæŒ‡å®šmodules_to_saveï¼Œè¿™äº›æ¨¡å—ä¼šå…¨é‡è®­ç»ƒå¹¶ä¿å­˜
        modules_to_save=["score"],
    )
    model = get_peft_model(model, lora_config)
    
    print("\nå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
    model.print_trainable_parameters()
    
    # éªŒè¯scoreå±‚ç¡®å®å¯è®­ç»ƒ
    score_params = list(model.score.parameters())
    print(f"âœ… scoreå±‚å‚æ•°æ•°é‡: {sum(p.numel() for p in score_params):,}")
    print(f"âœ… scoreå±‚æ˜¯å¦å¯è®­ç»ƒ: {score_params[0].requires_grad}")
    
    # 4. åŠ è½½æ•°æ®
    print("\nâ³ åŠ è½½åå¥½æ•°æ®...")
    raw_data = load_preference_data(DATA_PATH)
    dataset = Dataset.from_list(raw_data)
    
    print(f"\nğŸ“‹ æ•°æ®ç¤ºä¾‹:")
    print(f"   Prompt: {dataset[0]['prompt'][:40]}...")
    print(f"   Chosen: {dataset[0]['chosen'][:40]}...")
    print(f"   Rejected: {dataset[0]['rejected'][:40]}...")
    
    # 5. é…ç½®è®­ç»ƒ
    print("\nâ³ é…ç½®è®­ç»ƒ...")
    reward_config = RewardConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=4,
        learning_rate=LEARNING_RATE,
        logging_steps=2,
        save_steps=10,
        bf16=False,  # ç”¨float32æ›´ç¨³å®š
        fp16=False,
        report_to="none",
        max_length=MAX_LENGTH,
    )
    
    # 6. åˆ›å»ºRewardTrainer
    trainer = RewardTrainer(
        model=model,
        processing_class=tokenizer,
        train_dataset=dataset,
        args=reward_config,
    )
    print("âœ… RewardTraineråˆ›å»ºå®Œæˆ")
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("\nğŸ¬ å¼€å§‹è®­ç»ƒ Reward Modelï¼")
    print("ğŸ“Š åŸç†: chosençš„åˆ†æ•°è¦ > rejectedçš„åˆ†æ•°")
    trainer.train()
    
    # 8. å…³é”®ï¼šä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŒ…æ‹¬LoRAå’Œå…¨é‡è®­ç»ƒçš„scoreå±‚ï¼‰
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    
    # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(OUTPUT_DIR, "temp_save")
    trainer.save_model(temp_dir)
    
    # æ‰‹åŠ¨åˆå¹¶å¹¶ä¿å­˜ï¼ˆç¡®ä¿scoreå±‚è¢«æ­£ç¡®ä¿å­˜ï¼‰
    print("   åˆå¹¶LoRAæƒé‡...")
    merged_model = model.merge_and_unload()  # åˆå¹¶LoRAåˆ°åŸºç¡€æ¨¡å‹
    
    final_dir = os.path.join(OUTPUT_DIR, "final")
    os.makedirs(final_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹
    merged_model.save_pretrained(final_dir)
    tokenizer.save_pretrained(final_dir)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_info = {
        "base_model": BASE_MODEL,
        "lora_rank": LORA_RANK,
        "num_epochs": NUM_EPOCHS,
        "learning_rate": LEARNING_RATE,
        "data_size": len(raw_data)
    }
    with open(os.path.join(final_dir, "rm_config.json"), "w") as f:
        json.dump(config_info, f, indent=2)
    
    print(f"âœ… Reward Modelå·²ä¿å­˜åˆ°: {final_dir}")
    
    print("\nğŸ‰ Reward Modelè®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
